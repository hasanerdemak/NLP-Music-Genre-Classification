{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d93eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from statistics import mean\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "842debcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"../dataset/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "543d708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rap=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e77a16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rap=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rap']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "rap_sarki_sozleri=[]\n",
    "for index,row in df_rap.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    rap_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rap_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "\n",
    "rap_df = pd.DataFrame(data)\n",
    "rap_df[\"eski_turkce_kelime_orani\"] = df_rap[\"eski_turkce_kelime_orani\"].values\n",
    "rap_df[\"fiil_orani\"] = df_rap[\"fiil_orani\"].values\n",
    "rap_df[\"sifat_orani\"] = df_rap[\"sifat_orani\"].values\n",
    "rap_df[\"zarf_orani\"] = df_rap[\"zarf_orani\"].values\n",
    "rap_df[\"baglac_orani\"] = df_rap[\"baglac_orani\"].values\n",
    "rap_df[\"unlem_orani\"] = df_rap[\"unlem_orani\"].values\n",
    "rap_df[\"soru_orani\"] = df_rap[\"soru_orani\"].values\n",
    "rap_df[\"bilinmeyen_orani\"] = df_rap[\"bilinmeyen_orani\"].values\n",
    "rap_df['Label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17d4d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop=df_dataset.loc[df_dataset[\"sarki_turu\"]=='pop']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "pop_sarki_sozleri=[]\n",
    "for index,row in df_pop.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    pop_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "pop_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "pop_df = pd.DataFrame(data)\n",
    "pop_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "pop_df[\"fiil_orani\"] = df_pop[\"fiil_orani\"].values\n",
    "pop_df[\"sifat_orani\"] = df_pop[\"sifat_orani\"].values\n",
    "pop_df[\"zarf_orani\"] = df_pop[\"zarf_orani\"].values\n",
    "pop_df[\"baglac_orani\"] = df_pop[\"baglac_orani\"].values\n",
    "pop_df[\"unlem_orani\"] = df_pop[\"unlem_orani\"].values\n",
    "pop_df[\"soru_orani\"] = df_pop[\"soru_orani\"].values\n",
    "pop_df[\"bilinmeyen_orani\"] = df_pop[\"bilinmeyen_orani\"].values\n",
    "pop_df['Label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0749a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rock=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rock']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "rock_sarki_sozleri=[]\n",
    "for index,row in df_rock.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    rock_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rock_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "rock_df = pd.DataFrame(data)\n",
    "rock_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "rock_df[\"fiil_orani\"] = df_rock[\"fiil_orani\"].values\n",
    "rock_df[\"sifat_orani\"] = df_rock[\"sifat_orani\"].values\n",
    "rock_df[\"zarf_orani\"] = df_rock[\"zarf_orani\"].values\n",
    "rock_df[\"baglac_orani\"] = df_rock[\"baglac_orani\"].values\n",
    "rock_df[\"unlem_orani\"] = df_rock[\"unlem_orani\"].values\n",
    "rock_df[\"soru_orani\"] = df_rock[\"soru_orani\"].values\n",
    "rock_df[\"bilinmeyen_orani\"] = df_rock[\"bilinmeyen_orani\"].values\n",
    "rock_df['Label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae94082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ilahi=df_dataset.loc[df_dataset[\"sarki_turu\"]=='ilahi']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "ilahi_sarki_sozleri=[]\n",
    "for index,row in df_ilahi.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    ilahi_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "ilahi_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "ilahi_df = pd.DataFrame(data)\n",
    "ilahi_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "ilahi_df[\"fiil_orani\"] = df_ilahi[\"fiil_orani\"].values\n",
    "ilahi_df[\"sifat_orani\"] = df_ilahi[\"sifat_orani\"].values\n",
    "ilahi_df[\"zarf_orani\"] = df_ilahi[\"zarf_orani\"].values\n",
    "ilahi_df[\"baglac_orani\"] = df_ilahi[\"baglac_orani\"].values\n",
    "ilahi_df[\"unlem_orani\"] = df_ilahi[\"unlem_orani\"].values\n",
    "ilahi_df[\"soru_orani\"] = df_ilahi[\"soru_orani\"].values\n",
    "ilahi_df[\"bilinmeyen_orani\"] = df_ilahi[\"bilinmeyen_orani\"].values\n",
    "ilahi_df['Label']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f192ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanat=df_dataset.loc[df_dataset[\"sarki_turu\"]=='sanat']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "sanat_sarki_sozleri=[]\n",
    "for index,row in df_sanat.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    sanat_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "sanat_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "sanat_df = pd.DataFrame(data)\n",
    "sanat_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "sanat_df[\"fiil_orani\"] = df_sanat[\"fiil_orani\"].values\n",
    "sanat_df[\"sifat_orani\"] = df_sanat[\"sifat_orani\"].values\n",
    "sanat_df[\"zarf_orani\"] = df_sanat[\"zarf_orani\"].values\n",
    "sanat_df[\"baglac_orani\"] = df_sanat[\"baglac_orani\"].values\n",
    "sanat_df[\"unlem_orani\"] = df_sanat[\"unlem_orani\"].values\n",
    "sanat_df[\"soru_orani\"] = df_sanat[\"soru_orani\"].values\n",
    "sanat_df[\"bilinmeyen_orani\"] = df_sanat[\"bilinmeyen_orani\"].values\n",
    "sanat_df['Label']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "872ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_halk=df_dataset.loc[df_dataset[\"sarki_turu\"]=='halk']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "halk_sarki_sozleri=[]\n",
    "for index,row in df_halk.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    halk_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "halk_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "halk_df = pd.DataFrame(data)\n",
    "halk_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "halk_df[\"fiil_orani\"] = df_halk[\"fiil_orani\"].values\n",
    "halk_df[\"sifat_orani\"] = df_halk[\"sifat_orani\"].values\n",
    "halk_df[\"zarf_orani\"] = df_halk[\"zarf_orani\"].values\n",
    "halk_df[\"baglac_orani\"] = df_halk[\"baglac_orani\"].values\n",
    "halk_df[\"unlem_orani\"] = df_halk[\"unlem_orani\"].values\n",
    "halk_df[\"soru_orani\"] = df_halk[\"soru_orani\"].values\n",
    "halk_df[\"bilinmeyen_orani\"] = df_halk[\"bilinmeyen_orani\"].values\n",
    "halk_df['Label']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dc5f7",
   "metadata": {},
   "source": [
    "# Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc6443d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((200,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((200,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((200,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((200,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((200,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((200,))*5\n",
    "\n",
    "X = np.concatenate((data1, data2, data3,data4, data5, data6))\n",
    "y = np.concatenate((rep_label, pop_label, rock_label,ilahi_label, sanat_label, halk_label))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=38)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "svm = SVC(kernel='linear', random_state=0)\n",
    "svm.fit(X, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9283c",
   "metadata": {},
   "source": [
    "# Base Model Test and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3942bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 4. 5. 2. 1. 2. 1. 2. 1. 0. 2. 0. 0. 0. 0. 3. 0. 1. 2. 0. 5. 2. 4. 5.\n",
      " 5. 3. 5. 3. 4. 5. 0. 5. 1. 1. 5. 5. 2. 0. 5. 3. 3. 1. 3. 2. 5. 4. 4. 2.\n",
      " 2. 2. 5. 4. 4. 3. 3. 3. 4. 0. 4. 5. 0. 2. 1. 1. 3. 4. 3. 2. 5. 0. 3. 2.\n",
      " 0. 5. 3. 5. 5. 1. 3. 1. 3. 0. 2. 0. 0. 4. 4. 0. 4. 5. 4. 4. 0. 2. 4. 0.\n",
      " 3. 5. 4. 4. 5. 3. 2. 1. 4. 0. 0. 1. 0. 2. 4. 0. 5. 2. 3. 5. 4. 2. 4. 5.]\n",
      "reel: [0. 4. 4. 4. 1. 2. 1. 2. 1. 0. 1. 0. 0. 0. 0. 4. 0. 1. 2. 0. 5. 1. 4. 5.\n",
      " 5. 3. 4. 3. 5. 5. 2. 5. 1. 1. 4. 4. 2. 0. 1. 3. 3. 1. 1. 1. 4. 4. 5. 1.\n",
      " 2. 2. 4. 4. 4. 3. 2. 1. 4. 0. 4. 5. 2. 2. 1. 1. 3. 4. 3. 2. 4. 2. 3. 0.\n",
      " 2. 5. 3. 5. 5. 1. 5. 1. 3. 1. 2. 2. 1. 4. 4. 0. 4. 4. 4. 4. 0. 5. 4. 2.\n",
      " 0. 4. 4. 4. 4. 3. 2. 1. 1. 0. 0. 1. 0. 2. 4. 0. 5. 1. 3. 4. 4. 2. 4. 1.]\n",
      "Accuracy: 0.6833333333333333\n",
      "Precision: 0.7351331975772766\n",
      "Recall: 0.6833333333333333\n",
      "F1 score: 0.6816838112452147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_test_after_NGram = vectorizer.transform(X_test)\n",
    "y_pred = svm.predict(X_test_after_NGram)\n",
    "\n",
    "print(\"Predictions:\", y_test)\n",
    "print(\"reel:\", y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbc72a",
   "metadata": {},
   "source": [
    "# Makine Öğrenmesi Modelleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eec2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((200,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((200,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((200,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((200,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((200,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((200,))*5\n",
    "rap_df['sarki_sozu']=data1\n",
    "pop_df['sarki_sozu']=data2\n",
    "rock_df['sarki_sozu']=data3\n",
    "ilahi_df['sarki_sozu']=data4\n",
    "sanat_df['sarki_sozu']=data5\n",
    "halk_df['sarki_sozu']=data6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e470a",
   "metadata": {},
   "source": [
    "Duygu analizi özniteliği çıkarıldığı durumda model daha kötü performans sergilemiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b159d0",
   "metadata": {},
   "source": [
    "# SVM CLASİFİER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d31ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90        28\n",
      "           1       0.83      0.21      0.33        24\n",
      "           2       1.00      0.22      0.36        18\n",
      "           3       0.54      0.50      0.52        14\n",
      "           4       0.42      0.58      0.49        19\n",
      "           5       0.33      0.76      0.46        17\n",
      "\n",
      "    accuracy                           0.56       120\n",
      "   macro avg       0.66      0.54      0.51       120\n",
      "weighted avg       0.69      0.56      0.53       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "\n",
    "best_svm_clf = SVC(random_state=0)\n",
    "\n",
    "best_svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45835b8",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a11ec532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.96      0.59        28\n",
      "           1       0.42      0.21      0.28        24\n",
      "           2       0.62      0.28      0.38        18\n",
      "           3       0.81      0.93      0.87        14\n",
      "           4       0.67      0.11      0.18        19\n",
      "           5       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.54       120\n",
      "   macro avg       0.61      0.54      0.51       120\n",
      "weighted avg       0.58      0.54      0.49       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "# Fitting Naive Bayes to the Training set  \n",
    "\n",
    "best_NB_clf = GaussianNB()  \n",
    "best_NB_clf.fit(X_train, y_train)  \n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_NB_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f8a4a",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de1a3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        28\n",
      "           1       0.67      0.50      0.57        24\n",
      "           2       0.53      0.56      0.54        18\n",
      "           3       0.67      0.71      0.69        14\n",
      "           4       0.56      0.53      0.54        19\n",
      "           5       0.64      0.82      0.72        17\n",
      "\n",
      "    accuracy                           0.69       120\n",
      "   macro avg       0.67      0.68      0.67       120\n",
      "weighted avg       0.69      0.69      0.69       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "best_xgb_clf = xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "# Train the model on the training data\n",
    "best_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_xgb_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dbbf3",
   "metadata": {},
   "source": [
    "# Random Forest Without Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67fe9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        39\n",
      "           1       0.68      0.56      0.61        48\n",
      "           2       0.80      0.57      0.67        35\n",
      "           3       0.93      0.74      0.82        38\n",
      "           4       0.66      0.73      0.69        45\n",
      "           5       0.65      0.94      0.77        35\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.77      0.76      0.75       240\n",
      "weighted avg       0.76      0.75      0.75       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Duygu Analizi','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4cde9",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASİFİER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89adee",
   "metadata": {},
   "source": [
    "GridSearchCV Kullanılarak parametreler optimize edilmiştir.\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200,300,400,500,600],\n",
    "    'max_depth': [None, 10, 20, 30,40,50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=1, verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Sonucunda\n",
    "\n",
    "Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 600}\n",
    "\n",
    "Çıkmıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9292b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        39\n",
      "           1       0.74      0.58      0.65        48\n",
      "           2       0.77      0.57      0.66        35\n",
      "           3       0.97      0.76      0.85        38\n",
      "           4       0.64      0.78      0.70        45\n",
      "           5       0.65      0.91      0.76        35\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.78      0.77      0.76       240\n",
      "weighted avg       0.78      0.76      0.76       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
