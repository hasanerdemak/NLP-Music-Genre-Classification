{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "91d93eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from statistics import mean\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "842debcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv(\"../dataset/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d9e932d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>baslik</th>\n",
       "      <th>sarki_sozu</th>\n",
       "      <th>sarki_turu</th>\n",
       "      <th>eski_turkce_kelime_orani</th>\n",
       "      <th>fiil_orani</th>\n",
       "      <th>sifat_orani</th>\n",
       "      <th>zarf_orani</th>\n",
       "      <th>baglac_orani</th>\n",
       "      <th>unlem_orani</th>\n",
       "      <th>soru_orani</th>\n",
       "      <th>bilinmeyen_orani</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Acem Kızı</td>\n",
       "      <td>çırpınıp da şan ovaya çıkınca\\r\\neğlen şan ova...</td>\n",
       "      <td>halk</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Açma zülüflerin yellere karşı</td>\n",
       "      <td>açma zülüflerin yellere karşı\\r\\nsenin zülfün ...</td>\n",
       "      <td>halk</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ah bir ataş ver cigaramı yakayım</td>\n",
       "      <td>ah bir ataş ver cigaramı yakayım\\r\\nsen salın ...</td>\n",
       "      <td>halk</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sana olan duyguları bir bilebilsen (Ah le yar ...</td>\n",
       "      <td>sana olan duyguları bir bilebilsen\\r\\nanlayabi...</td>\n",
       "      <td>halk</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ahu gözlerini sevdiğim dilber</td>\n",
       "      <td>ahu gözlerini sevdiğim dilber\\r\\nsana bir sözü...</td>\n",
       "      <td>halk</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>ez - BENIM HAYALLER</td>\n",
       "      <td>heute eau de parfum in der goyardbag\\r\\nkomme ...</td>\n",
       "      <td>rap</td>\n",
       "      <td>0.072848</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>Canbay &amp; Wolker - Fersah</td>\n",
       "      <td>heyhat kafam göçtü diyardan\\r\\ndönmedi kimse l...</td>\n",
       "      <td>rap</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1197</td>\n",
       "      <td>grogi - balerin</td>\n",
       "      <td>yo\\r\\ni̇zmir\\r\\ngrogi\\r\\nanıl piyancı\\r\\ni̇zmi...</td>\n",
       "      <td>rap</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1198</td>\n",
       "      <td>sansar salvo ağır roman</td>\n",
       "      <td>sansar salvo yerinde duramayan adam\\r\\nbi çok ...</td>\n",
       "      <td>rap</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1199</td>\n",
       "      <td>Ceza - Med Cezir</td>\n",
       "      <td>sair olmak isteyen bir gezgin\\r\\nyasama hevesi...</td>\n",
       "      <td>rap</td>\n",
       "      <td>0.115888</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             baslik  \\\n",
       "0        0                                          Acem Kızı   \n",
       "1        1                      Açma zülüflerin yellere karşı   \n",
       "2        2                   Ah bir ataş ver cigaramı yakayım   \n",
       "3        3  Sana olan duyguları bir bilebilsen (Ah le yar ...   \n",
       "4        4                      Ahu gözlerini sevdiğim dilber   \n",
       "...    ...                                                ...   \n",
       "1195  1195                                ez - BENIM HAYALLER   \n",
       "1196  1196                           Canbay & Wolker - Fersah   \n",
       "1197  1197                                    grogi - balerin   \n",
       "1198  1198                            sansar salvo ağır roman   \n",
       "1199  1199                                   Ceza - Med Cezir   \n",
       "\n",
       "                                             sarki_sozu sarki_turu  \\\n",
       "0     çırpınıp da şan ovaya çıkınca\\r\\neğlen şan ova...       halk   \n",
       "1     açma zülüflerin yellere karşı\\r\\nsenin zülfün ...       halk   \n",
       "2     ah bir ataş ver cigaramı yakayım\\r\\nsen salın ...       halk   \n",
       "3     sana olan duyguları bir bilebilsen\\r\\nanlayabi...       halk   \n",
       "4     ahu gözlerini sevdiğim dilber\\r\\nsana bir sözü...       halk   \n",
       "...                                                 ...        ...   \n",
       "1195  heute eau de parfum in der goyardbag\\r\\nkomme ...        rap   \n",
       "1196  heyhat kafam göçtü diyardan\\r\\ndönmedi kimse l...        rap   \n",
       "1197  yo\\r\\ni̇zmir\\r\\ngrogi\\r\\nanıl piyancı\\r\\ni̇zmi...        rap   \n",
       "1198  sansar salvo yerinde duramayan adam\\r\\nbi çok ...        rap   \n",
       "1199  sair olmak isteyen bir gezgin\\r\\nyasama hevesi...        rap   \n",
       "\n",
       "      eski_turkce_kelime_orani  fiil_orani  sifat_orani  zarf_orani  \\\n",
       "0                     0.204545        0.07         0.05        0.09   \n",
       "1                     0.148936        0.15         0.00        0.00   \n",
       "2                     0.176471        0.28         0.08        0.00   \n",
       "3                     0.241935        0.16         0.04        0.05   \n",
       "4                     0.111111        0.25         0.13        0.02   \n",
       "...                        ...         ...          ...         ...   \n",
       "1195                  0.072848        0.10         0.05        0.05   \n",
       "1196                  0.023411        0.12         0.06        0.03   \n",
       "1197                  0.018617        0.16         0.10        0.04   \n",
       "1198                  0.065972        0.15         0.14        0.03   \n",
       "1199                  0.115888        0.10         0.08        0.04   \n",
       "\n",
       "      baglac_orani  unlem_orani  soru_orani  bilinmeyen_orani  \n",
       "0             0.02         0.00        0.05              0.00  \n",
       "1             0.00         0.00        0.07              0.02  \n",
       "2             0.00         0.00        0.00              0.02  \n",
       "3             0.04         0.00        0.00              0.01  \n",
       "4             0.00         0.00        0.02              0.04  \n",
       "...            ...          ...         ...               ...  \n",
       "1195          0.03         0.00        0.01              0.27  \n",
       "1196          0.01         0.02        0.00              0.07  \n",
       "1197          0.02         0.01        0.00              0.15  \n",
       "1198          0.05         0.00        0.00              0.05  \n",
       "1199          0.03         0.00        0.00              0.29  \n",
       "\n",
       "[1200 rows x 12 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "543d708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rap=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e77a16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rap=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rap']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "rap_sarki_sozleri=[]\n",
    "for index,row in df_rap.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    rap_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rap_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "\n",
    "rap_df = pd.DataFrame(data)\n",
    "rap_df[\"eski_turkce_kelime_orani\"] = df_rap[\"eski_turkce_kelime_orani\"].values\n",
    "rap_df[\"fiil_orani\"] = df_rap[\"fiil_orani\"].values\n",
    "rap_df[\"sifat_orani\"] = df_rap[\"sifat_orani\"].values\n",
    "rap_df[\"zarf_orani\"] = df_rap[\"zarf_orani\"].values\n",
    "rap_df[\"baglac_orani\"] = df_rap[\"baglac_orani\"].values\n",
    "rap_df[\"unlem_orani\"] = df_rap[\"unlem_orani\"].values\n",
    "rap_df[\"soru_orani\"] = df_rap[\"soru_orani\"].values\n",
    "rap_df[\"bilinmeyen_orani\"] = df_rap[\"bilinmeyen_orani\"].values\n",
    "rap_df['Label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5418645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Şarki Adı</th>\n",
       "      <th>Toplam Kelime Sayısı</th>\n",
       "      <th>Ortalama Kelime Uzunlukları</th>\n",
       "      <th>Ortalama Kelime Tekrarı</th>\n",
       "      <th>Duygu Analizi</th>\n",
       "      <th>eski_turkce_kelime_orani</th>\n",
       "      <th>fiil_orani</th>\n",
       "      <th>sifat_orani</th>\n",
       "      <th>zarf_orani</th>\n",
       "      <th>baglac_orani</th>\n",
       "      <th>unlem_orani</th>\n",
       "      <th>soru_orani</th>\n",
       "      <th>bilinmeyen_orani</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sagopa Kajmer - Istakoz</td>\n",
       "      <td>346</td>\n",
       "      <td>6.624277</td>\n",
       "      <td>1.572491</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ezhel - Felaket</td>\n",
       "      <td>120</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>1.387931</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118012</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sansar Salvo - Bela</td>\n",
       "      <td>125</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1.424779</td>\n",
       "      <td>3</td>\n",
       "      <td>0.055901</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norm Ender - Mekanın Sahibi</td>\n",
       "      <td>241</td>\n",
       "      <td>7.012448</td>\n",
       "      <td>1.464115</td>\n",
       "      <td>2</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ben Fero - Mahallemiz Esmer</td>\n",
       "      <td>266</td>\n",
       "      <td>6.796992</td>\n",
       "      <td>1.429864</td>\n",
       "      <td>2</td>\n",
       "      <td>0.060127</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ez - BENIM HAYALLER</td>\n",
       "      <td>248</td>\n",
       "      <td>6.217742</td>\n",
       "      <td>1.589474</td>\n",
       "      <td>2</td>\n",
       "      <td>0.072848</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Canbay &amp; Wolker - Fersah</td>\n",
       "      <td>239</td>\n",
       "      <td>7.380753</td>\n",
       "      <td>1.661111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>grogi - balerin</td>\n",
       "      <td>310</td>\n",
       "      <td>6.587097</td>\n",
       "      <td>1.928205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018617</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>sansar salvo ağır roman</td>\n",
       "      <td>240</td>\n",
       "      <td>7.158333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Ceza - Med Cezir</td>\n",
       "      <td>413</td>\n",
       "      <td>8.292978</td>\n",
       "      <td>1.693038</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115888</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Şarki Adı  Toplam Kelime Sayısı  \\\n",
       "0        Sagopa Kajmer - Istakoz                   346   \n",
       "1                Ezhel - Felaket                   120   \n",
       "2            Sansar Salvo - Bela                   125   \n",
       "3    Norm Ender - Mekanın Sahibi                   241   \n",
       "4    Ben Fero - Mahallemiz Esmer                   266   \n",
       "..                           ...                   ...   \n",
       "195          ez - BENIM HAYALLER                   248   \n",
       "196     Canbay & Wolker - Fersah                   239   \n",
       "197              grogi - balerin                   310   \n",
       "198      sansar salvo ağır roman                   240   \n",
       "199             Ceza - Med Cezir                   413   \n",
       "\n",
       "     Ortalama Kelime Uzunlukları  Ortalama Kelime Tekrarı   Duygu Analizi  \\\n",
       "0                       6.624277                  1.572491              0   \n",
       "1                       8.166667                  1.387931              0   \n",
       "2                       7.400000                  1.424779              3   \n",
       "3                       7.012448                  1.464115              2   \n",
       "4                       6.796992                  1.429864              2   \n",
       "..                           ...                       ...            ...   \n",
       "195                     6.217742                  1.589474              2   \n",
       "196                     7.380753                  1.661111              0   \n",
       "197                     6.587097                  1.928205              0   \n",
       "198                     7.158333                  1.500000              0   \n",
       "199                     8.292978                  1.693038              0   \n",
       "\n",
       "     eski_turkce_kelime_orani  fiil_orani  sifat_orani  zarf_orani  \\\n",
       "0                    0.070922        0.17         0.09        0.05   \n",
       "1                    0.118012        0.19         0.06        0.08   \n",
       "2                    0.055901        0.25         0.08        0.04   \n",
       "3                    0.013072        0.16         0.08        0.04   \n",
       "4                    0.060127        0.13         0.07        0.04   \n",
       "..                        ...         ...          ...         ...   \n",
       "195                  0.072848        0.10         0.05        0.05   \n",
       "196                  0.023411        0.12         0.06        0.03   \n",
       "197                  0.018617        0.16         0.10        0.04   \n",
       "198                  0.065972        0.15         0.14        0.03   \n",
       "199                  0.115888        0.10         0.08        0.04   \n",
       "\n",
       "     baglac_orani  unlem_orani  soru_orani  bilinmeyen_orani  Label  \n",
       "0            0.05         0.01        0.00              0.03      0  \n",
       "1            0.02         0.00        0.00              0.03      0  \n",
       "2            0.03         0.00        0.01              0.06      0  \n",
       "3            0.02         0.07        0.00              0.11      0  \n",
       "4            0.01         0.01        0.01              0.12      0  \n",
       "..            ...          ...         ...               ...    ...  \n",
       "195          0.03         0.00        0.01              0.27      0  \n",
       "196          0.01         0.02        0.00              0.07      0  \n",
       "197          0.02         0.01        0.00              0.15      0  \n",
       "198          0.05         0.00        0.00              0.05      0  \n",
       "199          0.03         0.00        0.00              0.29      0  \n",
       "\n",
       "[200 rows x 14 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "17d4d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop=df_dataset.loc[df_dataset[\"sarki_turu\"]=='pop']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "pop_sarki_sozleri=[]\n",
    "for index,row in df_pop.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    pop_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "pop_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "pop_df = pd.DataFrame(data)\n",
    "pop_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "pop_df[\"fiil_orani\"] = df_pop[\"fiil_orani\"].values\n",
    "pop_df[\"sifat_orani\"] = df_pop[\"sifat_orani\"].values\n",
    "pop_df[\"zarf_orani\"] = df_pop[\"zarf_orani\"].values\n",
    "pop_df[\"baglac_orani\"] = df_pop[\"baglac_orani\"].values\n",
    "pop_df[\"unlem_orani\"] = df_pop[\"unlem_orani\"].values\n",
    "pop_df[\"soru_orani\"] = df_pop[\"soru_orani\"].values\n",
    "pop_df[\"bilinmeyen_orani\"] = df_pop[\"bilinmeyen_orani\"].values\n",
    "pop_df['Label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0749a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rock=df_dataset.loc[df_dataset[\"sarki_turu\"]=='rock']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "rock_sarki_sozleri=[]\n",
    "for index,row in df_rock.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    rock_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rock_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "rock_df = pd.DataFrame(data)\n",
    "rock_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "rock_df[\"fiil_orani\"] = df_rock[\"fiil_orani\"].values\n",
    "rock_df[\"sifat_orani\"] = df_rock[\"sifat_orani\"].values\n",
    "rock_df[\"zarf_orani\"] = df_rock[\"zarf_orani\"].values\n",
    "rock_df[\"baglac_orani\"] = df_rock[\"baglac_orani\"].values\n",
    "rock_df[\"unlem_orani\"] = df_rock[\"unlem_orani\"].values\n",
    "rock_df[\"soru_orani\"] = df_rock[\"soru_orani\"].values\n",
    "rock_df[\"bilinmeyen_orani\"] = df_rock[\"bilinmeyen_orani\"].values\n",
    "rock_df['Label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9ae94082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ilahi=df_dataset.loc[df_dataset[\"sarki_turu\"]=='ilahi']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "ilahi_sarki_sozleri=[]\n",
    "for index,row in df_ilahi.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    ilahi_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "ilahi_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "ilahi_df = pd.DataFrame(data)\n",
    "ilahi_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "ilahi_df[\"fiil_orani\"] = df_ilahi[\"fiil_orani\"].values\n",
    "ilahi_df[\"sifat_orani\"] = df_ilahi[\"sifat_orani\"].values\n",
    "ilahi_df[\"zarf_orani\"] = df_ilahi[\"zarf_orani\"].values\n",
    "ilahi_df[\"baglac_orani\"] = df_ilahi[\"baglac_orani\"].values\n",
    "ilahi_df[\"unlem_orani\"] = df_ilahi[\"unlem_orani\"].values\n",
    "ilahi_df[\"soru_orani\"] = df_ilahi[\"soru_orani\"].values\n",
    "ilahi_df[\"bilinmeyen_orani\"] = df_ilahi[\"bilinmeyen_orani\"].values\n",
    "ilahi_df['Label']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f192ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sanat=df_dataset.loc[df_dataset[\"sarki_turu\"]=='sanat']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "sanat_sarki_sozleri=[]\n",
    "for index,row in df_sanat.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    sanat_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "sanat_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "sanat_df = pd.DataFrame(data)\n",
    "sanat_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "sanat_df[\"fiil_orani\"] = df_sanat[\"fiil_orani\"].values\n",
    "sanat_df[\"sifat_orani\"] = df_sanat[\"sifat_orani\"].values\n",
    "sanat_df[\"zarf_orani\"] = df_sanat[\"zarf_orani\"].values\n",
    "sanat_df[\"baglac_orani\"] = df_sanat[\"baglac_orani\"].values\n",
    "sanat_df[\"unlem_orani\"] = df_sanat[\"unlem_orani\"].values\n",
    "sanat_df[\"soru_orani\"] = df_sanat[\"soru_orani\"].values\n",
    "sanat_df[\"bilinmeyen_orani\"] = df_sanat[\"bilinmeyen_orani\"].values\n",
    "sanat_df['Label']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "872ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_halk=df_dataset.loc[df_dataset[\"sarki_turu\"]=='halk']\n",
    "\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "halk_sarki_sozleri=[]\n",
    "for index,row in df_halk.iterrows():\n",
    "    sarki_adi.append(row['baslik'])\n",
    "    halk_sarki_sozleri.append(row['sarki_sozu'])\n",
    "    #row=\"\".join(row)\n",
    "   \n",
    "    lyrics = row[\"sarki_sozu\"]\n",
    "    lengths.append(len(lyrics.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in lyrics.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row[\"sarki_sozu\"]\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=lyrics.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "halk_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "halk_df = pd.DataFrame(data)\n",
    "halk_df[\"eski_turkce_kelime_orani\"] = df_pop[\"eski_turkce_kelime_orani\"].values\n",
    "halk_df[\"fiil_orani\"] = df_halk[\"fiil_orani\"].values\n",
    "halk_df[\"sifat_orani\"] = df_halk[\"sifat_orani\"].values\n",
    "halk_df[\"zarf_orani\"] = df_halk[\"zarf_orani\"].values\n",
    "halk_df[\"baglac_orani\"] = df_halk[\"baglac_orani\"].values\n",
    "halk_df[\"unlem_orani\"] = df_halk[\"unlem_orani\"].values\n",
    "halk_df[\"soru_orani\"] = df_halk[\"soru_orani\"].values\n",
    "halk_df[\"bilinmeyen_orani\"] = df_halk[\"bilinmeyen_orani\"].values\n",
    "halk_df['Label']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dc5f7",
   "metadata": {},
   "source": [
    "# Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fc6443d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((200,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((200,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((200,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((200,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((200,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((200,))*5\n",
    "\n",
    "X = np.concatenate((data1, data2, data3,data4, data5, data6))\n",
    "y = np.concatenate((rep_label, pop_label, rock_label,ilahi_label, sanat_label, halk_label))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=38)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=0)\n",
    "svm.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9283c",
   "metadata": {},
   "source": [
    "# Base Model Test and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3942bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 4. 5. 2. 1. 2. 1. 2. 1. 0. 2. 0. 0. 0. 0. 3. 0. 1. 2. 0. 5. 2. 4. 5.\n",
      " 5. 3. 5. 3. 4. 5. 0. 5. 1. 1. 5. 5. 2. 0. 5. 3. 3. 1. 3. 2. 5. 4. 4. 2.\n",
      " 2. 2. 5. 4. 4. 3. 3. 3. 4. 0. 4. 5. 0. 2. 1. 1. 3. 4. 3. 2. 5. 0. 3. 2.\n",
      " 0. 5. 3. 5. 5. 1. 3. 1. 3. 0. 2. 0. 0. 4. 4. 0. 4. 5. 4. 4. 0. 2. 4. 0.\n",
      " 3. 5. 4. 4. 5. 3. 2. 1. 4. 0. 0. 1. 0. 2. 4. 0. 5. 2. 3. 5. 4. 2. 4. 5.]\n",
      "reel: [4. 4. 4. 4. 4. 2. 4. 2. 4. 4. 1. 4. 4. 0. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 3. 4. 4. 5. 4. 1. 4. 4. 4. 4. 4. 4. 1. 4. 4. 4. 4. 1. 4. 4. 4. 4. 4.\n",
      " 2. 1. 4. 4. 4. 4. 4. 4. 4. 1. 4. 5. 4. 2. 4. 4. 4. 4. 4. 2. 4. 4. 3. 1.\n",
      " 1. 4. 4. 4. 4. 4. 4. 4. 4. 1. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 5. 4. 4.\n",
      " 4. 4. 4. 4. 4. 3. 2. 4. 4. 4. 4. 4. 4. 1. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n",
      "Accuracy: 0.26666666666666666\n",
      "Precision: 0.6202462772050402\n",
      "Recall: 0.26666666666666666\n",
      "F1 score: 0.21522969187675073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_test_after_NGram = vectorizer.transform(X_test)\n",
    "y_pred = svm.predict(X_test_after_NGram)\n",
    "\n",
    "print(\"Predictions:\", y_test)\n",
    "print(\"reel:\", y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbc72a",
   "metadata": {},
   "source": [
    "# Makine Öğrenmesi Modelleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5eec2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((200,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((200,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((200,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((200,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((200,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((200,))*5\n",
    "rap_df['sarki_sozu']=data1\n",
    "pop_df['sarki_sozu']=data2\n",
    "rock_df['sarki_sozu']=data3\n",
    "ilahi_df['sarki_sozu']=data4\n",
    "sanat_df['sarki_sozu']=data5\n",
    "halk_df['sarki_sozu']=data6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dbbf3",
   "metadata": {},
   "source": [
    "# Random Forest Without Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "67fe9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        39\n",
      "           1       0.68      0.56      0.61        48\n",
      "           2       0.80      0.57      0.67        35\n",
      "           3       0.93      0.74      0.82        38\n",
      "           4       0.66      0.73      0.69        45\n",
      "           5       0.65      0.94      0.77        35\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.77      0.76      0.75       240\n",
      "weighted avg       0.76      0.75      0.75       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Duygu Analizi','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4cde9",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASİFİER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89adee",
   "metadata": {},
   "source": [
    "GridSearchCV Kullanılarak parametreler optimize edilmiştir.\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200,300,400,500,600],\n",
    "    'max_depth': [None, 10, 20, 30,40,50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=1, verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Sonucunda\n",
    "\n",
    "Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 600}\n",
    "\n",
    "Çıkmıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9292b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        39\n",
      "           1       0.78      0.58      0.67        48\n",
      "           2       0.78      0.60      0.68        35\n",
      "           3       0.97      0.76      0.85        38\n",
      "           4       0.63      0.80      0.71        45\n",
      "           5       0.65      0.89      0.75        35\n",
      "\n",
      "    accuracy                           0.77       240\n",
      "   macro avg       0.79      0.77      0.77       240\n",
      "weighted avg       0.79      0.77      0.76       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e470a",
   "metadata": {},
   "source": [
    "Duygu analizi özniteliği çıkarıldığı durumda model daha kötü performans sergilemiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b159d0",
   "metadata": {},
   "source": [
    "# SVM CLASİFİER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1d31ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90        28\n",
      "           1       0.83      0.21      0.33        24\n",
      "           2       1.00      0.22      0.36        18\n",
      "           3       0.54      0.50      0.52        14\n",
      "           4       0.42      0.58      0.49        19\n",
      "           5       0.33      0.76      0.46        17\n",
      "\n",
      "    accuracy                           0.56       120\n",
      "   macro avg       0.66      0.54      0.51       120\n",
      "weighted avg       0.69      0.56      0.53       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "\n",
    "best_svm_clf = SVC(random_state=0)\n",
    "\n",
    "best_svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_svm_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45835b8",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a11ec532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.96      0.59        28\n",
      "           1       0.42      0.21      0.28        24\n",
      "           2       0.62      0.28      0.38        18\n",
      "           3       0.81      0.93      0.87        14\n",
      "           4       0.67      0.11      0.18        19\n",
      "           5       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.54       120\n",
      "   macro avg       0.61      0.54      0.51       120\n",
      "weighted avg       0.58      0.54      0.49       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "# Fitting Naive Bayes to the Training set  \n",
    "\n",
    "best_NB_clf = GaussianNB()  \n",
    "best_NB_clf.fit(X_train, y_train)  \n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_NB_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f8a4a",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "de1a3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        28\n",
      "           1       0.67      0.50      0.57        24\n",
      "           2       0.50      0.56      0.53        18\n",
      "           3       0.77      0.71      0.74        14\n",
      "           4       0.56      0.53      0.54        19\n",
      "           5       0.65      0.88      0.75        17\n",
      "\n",
      "    accuracy                           0.70       120\n",
      "   macro avg       0.68      0.69      0.68       120\n",
      "weighted avg       0.70      0.70      0.70       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "best_xgb_clf = xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "# Train the model on the training data\n",
    "best_xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_xgb_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
