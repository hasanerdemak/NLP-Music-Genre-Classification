{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d93eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from statistics import mean\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77a16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('rap.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[] \n",
    "rap_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    rap_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "   \n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rap_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "rap_df = pd.DataFrame(data)\n",
    "rap_df=rap_df. tail(-1)\n",
    "rap_df['Label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d4d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('pop1.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[]  \n",
    "pop_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    pop_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "pop_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "pop_df = pd.DataFrame(data)\n",
    "pop_df=pop_df. tail(-1)\n",
    "pop_df['Label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0749a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('rock.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[]\n",
    "rock_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    rock_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "rock_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "rock_df = pd.DataFrame(data)\n",
    "rock_df=rock_df. tail(-1)\n",
    "rock_df['Label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae94082",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('ilahi.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[]\n",
    "ilahi_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    ilahi_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "\n",
    "ilahi_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "ilahi_df = pd.DataFrame(data)\n",
    "ilahi_df=ilahi_df. tail(-1)\n",
    "ilahi_df['Label']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f192ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('sanat.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[]\n",
    "sanat_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    sanat_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "    row=\"\".join(row)\n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "sanat_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "sanat_df = pd.DataFrame(data)\n",
    "sanat_df=sanat_df. tail(-1)\n",
    "sanat_df['Label']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('halk.csv',encoding='utf-8')\n",
    "csvreader = csv.reader(file)\n",
    "sarki_adi=[]\n",
    "lengths=[]\n",
    "word_lengths=[]\n",
    "repeated_word=[]\n",
    "emtion_analysis=[]\n",
    "halk_sarki_sozleri=[]\n",
    "for row in csvreader:\n",
    "    sarki_adi.append(row[0])\n",
    "    halk_sarki_sozleri.append(row[1])\n",
    "    row=\"\".join(row)\n",
    "    str_row=row\n",
    "    row = os.linesep.join([s for s in row.splitlines() if s])\n",
    "    lengths.append(len(row.split(\" \")))\n",
    "    temp=[]\n",
    "    for word in row.split(\" \"):\n",
    "        temp.append(len(word))\n",
    "\n",
    "    turkish_text = row\n",
    "    blob = TextBlob(turkish_text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    if polarity > 0:\n",
    "        if polarity >= 0.5:\n",
    "           # print(\"Mutlu\")\n",
    "            emtion_analysis.append(5)\n",
    "        else:\n",
    "            #print(\"Şaşırmış\")\n",
    "            emtion_analysis.append(2)\n",
    "    elif polarity < 0:\n",
    "        if polarity <= -0.5:\n",
    "            #print(\"Kızgın\")\n",
    "            emtion_analysis.append(3)\n",
    "        else:\n",
    "            #print(\"Üzgün\")\n",
    "            emtion_analysis.append(4)\n",
    "    else:\n",
    "        \n",
    "        if \"korku\" in turkish_text.lower() or \"korkmak\" in turkish_text.lower():\n",
    "            if polarity <= -0.3:\n",
    "                #print(\"Korkmuş\")\n",
    "                emtion_analysis.append(1)\n",
    "            else:\n",
    "                #print(\"Nötr\")\n",
    "                emtion_analysis.append(0)\n",
    "        else:\n",
    "            #print(\"Nötr\")\n",
    "            emtion_analysis.append(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    word_lengths.append(mean(temp))\n",
    "\n",
    "    counts = {}\n",
    "    str_row=str_row.split((\"\\n\"))\n",
    "    temp_List2=[]\n",
    "    for sentence in str_row:\n",
    "        for word in sentence.split(\" \"):\n",
    "            #print(word)\n",
    "            #print()\n",
    "\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "    for item in counts.values():\n",
    "        temp_List2.append(item)\n",
    "        #print(item)\n",
    "        #print()\n",
    "    #sorted_c=sorted(counts)\n",
    "    repeated_word.append(sum(temp_List2)/len(temp_List2))\n",
    "halk_plot_data=[mean(lengths),mean(word_lengths),mean(repeated_word)]\n",
    "data = {'Şarki Adı':sarki_adi,\n",
    "        'Toplam Kelime Sayısı': lengths,\n",
    "        'Ortalama Kelime Uzunlukları': word_lengths,\n",
    "        'Ortalama Kelime Tekrarı ':repeated_word,\n",
    "        'Duygu Analizi':emtion_analysis}\n",
    "halk_df = pd.DataFrame(data)\n",
    "halk_df=halk_df. tail(-1)\n",
    "halk_df['Label']=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dc5f7",
   "metadata": {},
   "source": [
    "# Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6443d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((201,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((201,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((201,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((201,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((201,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((201,))*5\n",
    "\n",
    "X = np.concatenate((data1, data2, data3,data4, data5, data6))\n",
    "y = np.concatenate((rep_label, pop_label, rock_label,ilahi_label, sanat_label, halk_label))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=38)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "svm = SVC(kernel='linear', random_state=0)\n",
    "svm.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9283c",
   "metadata": {},
   "source": [
    "# Base Model Test and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3942bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 0. 4. 2. 0. 0. 5. 3. 4. 3. 2. 2. 3. 4. 2. 3. 1. 5. 1. 2. 5. 1. 3. 0.\n",
      " 2. 2. 0. 1. 5. 5. 4. 5. 0. 4. 5. 2. 5. 1. 1. 3. 1. 3. 0. 0. 4. 5. 5. 0.\n",
      " 1. 1. 2. 2. 2. 4. 5. 3. 4. 1. 4. 4. 5. 3. 3. 0. 4. 0. 3. 0. 4. 3. 5. 3.\n",
      " 3. 4. 2. 0. 4. 0. 5. 4. 4. 4. 5. 3. 4. 0. 1. 0. 3. 0. 1. 0. 4. 1. 1. 0.\n",
      " 0. 1. 4. 2. 2. 2. 3. 2. 3. 4. 0. 4. 5. 2. 0. 3. 5. 2. 4. 0. 2. 5. 2. 5.\n",
      " 1.]\n",
      "reel: [1. 5. 4. 1. 4. 1. 4. 4. 4. 4. 1. 2. 3. 4. 1. 4. 4. 5. 1. 1. 4. 4. 1. 4.\n",
      " 2. 1. 1. 1. 4. 5. 4. 4. 4. 1. 4. 2. 4. 1. 1. 4. 4. 4. 0. 1. 4. 4. 4. 1.\n",
      " 4. 4. 4. 4. 1. 4. 5. 4. 4. 4. 1. 4. 4. 4. 4. 0. 4. 1. 3. 4. 4. 3. 4. 3.\n",
      " 4. 4. 1. 1. 4. 0. 4. 4. 4. 4. 4. 3. 4. 1. 1. 0. 4. 4. 4. 1. 4. 4. 4. 4.\n",
      " 4. 1. 4. 4. 1. 4. 4. 4. 1. 5. 4. 4. 4. 2. 4. 4. 4. 4. 4. 4. 2. 4. 5. 4.\n",
      " 4.]\n",
      "Accuracy: 0.35537190082644626\n",
      "Precision: 0.6799319981138162\n",
      "Recall: 0.35537190082644626\n",
      "F1 score: 0.3411856354161448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_test_after_NGram = vectorizer.transform(X_test)\n",
    "y_pred = svm.predict(X_test_after_NGram)\n",
    "\n",
    "print(\"Predictions:\", y_test)\n",
    "print(\"reel:\", y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbc72a",
   "metadata": {},
   "source": [
    "# Makine Öğrenmesi Modelleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eec2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "data1 = np.array(rap_sarki_sozleri)\n",
    "rep_label = np.ones((200,))*0\n",
    "\n",
    "\n",
    "# Dataset 2\n",
    "data2 = np.array(pop_sarki_sozleri)\n",
    "pop_label = np.ones((200,))\n",
    "\n",
    "# Dataset 3\n",
    "data3 = np.array(rock_sarki_sozleri)\n",
    "rock_label = np.ones((200,))*2\n",
    "\n",
    "# Dataset 4\n",
    "data4 = np.array(ilahi_sarki_sozleri)\n",
    "ilahi_label = np.ones((200,))*3\n",
    "\n",
    "# Dataset 5\n",
    "data5 = np.array(sanat_sarki_sozleri)\n",
    "sanat_label = np.ones((200,))*4\n",
    "\n",
    "# Dataset 6\n",
    "data6 = np.array(halk_sarki_sozleri)\n",
    "halk_label = np.ones((200,))*5\n",
    "rap_df['sarki_sozu']=data1[1:]\n",
    "pop_df['sarki_sozu']=data2[1:]\n",
    "rock_df['sarki_sozu']=data3[1:]\n",
    "ilahi_df['sarki_sozu']=data4[1:]\n",
    "sanat_df['sarki_sozu']=data5[1:]\n",
    "halk_df['sarki_sozu']=data6[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4cde9",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASİFİER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89adee",
   "metadata": {},
   "source": [
    "GridSearchCV Kullanılarak parametreler optimize edilmiştir.\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200,300,400,500,600],\n",
    "    'max_depth': [None, 10, 20, 30,40,50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv=1, verbose=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "Sonucunda\n",
    "\n",
    "Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 600}\n",
    "\n",
    "Çıkmıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9292b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        39\n",
      "           1       0.67      0.50      0.57        48\n",
      "           2       0.76      0.63      0.69        35\n",
      "           3       0.94      0.82      0.87        38\n",
      "           4       0.65      0.69      0.67        45\n",
      "           5       0.64      0.91      0.75        35\n",
      "\n",
      "    accuracy                           0.75       240\n",
      "   macro avg       0.76      0.76      0.75       240\n",
      "weighted avg       0.75      0.75      0.74       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dbbf3",
   "metadata": {},
   "source": [
    "# Random Forest Without Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67fe9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        39\n",
      "           1       0.64      0.52      0.57        48\n",
      "           2       0.74      0.57      0.65        35\n",
      "           3       0.91      0.79      0.85        38\n",
      "           4       0.61      0.62      0.62        45\n",
      "           5       0.61      0.89      0.72        35\n",
      "\n",
      "    accuracy                           0.72       240\n",
      "   macro avg       0.73      0.73      0.72       240\n",
      "weighted avg       0.73      0.72      0.72       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Duygu Analizi','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(max_depth= 30, min_samples_leaf= 1, min_samples_split= 5, \n",
    "                                     n_estimators=600, random_state=42)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e470a",
   "metadata": {},
   "source": [
    "Duygu analizi özniteliği çıkarıldığı durumda model daha kötü performans sergilemiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b159d0",
   "metadata": {},
   "source": [
    "# SVM CLASİFİER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d31ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90        28\n",
      "           1       0.83      0.21      0.33        24\n",
      "           2       1.00      0.22      0.36        18\n",
      "           3       0.54      0.50      0.52        14\n",
      "           4       0.41      0.58      0.48        19\n",
      "           5       0.32      0.71      0.44        17\n",
      "\n",
      "    accuracy                           0.55       120\n",
      "   macro avg       0.66      0.53      0.51       120\n",
      "weighted avg       0.69      0.55      0.53       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "\n",
    "best_rf_clf = SVC(random_state=0)\n",
    "\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45835b8",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11ec532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.96      0.59        28\n",
      "           1       0.46      0.25      0.32        24\n",
      "           2       0.57      0.22      0.32        18\n",
      "           3       0.81      0.93      0.87        14\n",
      "           4       0.67      0.11      0.18        19\n",
      "           5       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.54       120\n",
      "   macro avg       0.61      0.54      0.50       120\n",
      "weighted avg       0.58      0.54      0.49       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Train the RandomForest classifier with the best parameters\n",
    "\n",
    "# Fitting Naive Bayes to the Training set  \n",
    "\n",
    "best_rf_clf = GaussianNB()  \n",
    "best_rf_clf.fit(X_train, y_train)  \n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = best_rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f8a4a",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de1a3dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your data is in a CSV file called 'data.csv'\n",
    "datasets = [rap_df, pop_df, rock_df, ilahi_df, sanat_df, halk_df]\n",
    "data = pd.concat(datasets, ignore_index=True,axis=0)\n",
    "\n",
    "text_data = data['sarki_sozu']\n",
    "\n",
    "# Extract n-grams from the text data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Using both unigrams and bigrams\n",
    "text_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert n-grams features to a DataFrame\n",
    "text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Combine the n-gram features with the original features\n",
    "X = data.drop(['sarki_sozu','Şarki Adı','Label'], axis=1)  # Replace 'label' with the name of your label column\n",
    "X = pd.concat([X, text_features_df], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Separate the label\n",
    "y = data['Label']  # Replace 'label' with the name of your label column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "# Print the model's accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3279ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
